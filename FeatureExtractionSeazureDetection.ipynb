{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pywt\n",
    "import pathlib\n",
    "import pyedflib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyentrp import entropy\n",
    "from scipy import signal, stats, integrate\n",
    "from sklearn import metrics, preprocessing, decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDF Files Manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read an edf file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "https://stackoverflow.com/questions/48784257/convert-eye-tracking-edf-file-to-asc-csv-format\n",
    "https://pyedflib.readthedocs.io/en/latest/\n",
    "https://www.edfplus.info/specs/edf.html\n",
    "'''\n",
    "def readEdfFile(pathToFile):\n",
    "    f = pyedflib.EdfReader(pathToFile)\n",
    "    n = f.signals_in_file\n",
    "    signal_labels = f.getSignalLabels()\n",
    "    sigbufs = np.zeros((f.getNSamples()[0],n))\n",
    "    for i in np.arange(n):\n",
    "        sigbufs[:,i] = f.readSignal(i)\n",
    "    f._close()\n",
    "    del i,f,n,signal_labels\n",
    "    return sigbufs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read an edf seizures file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "https://www.mathworks.com/matlabcentral/answers/225716-how-i-can-read-chb01_03-edf-seizures-file-from-chb-mit-database-in-matlab-as-i-am-using-this-file-f\n",
    "Returns start time and length of the seizure\n",
    "'''\n",
    "def get_seizure_period(file_location):\n",
    "    bytes_array = []\n",
    "    for b in pathlib.Path(file_location).read_bytes(): bytes_array.append(b)\n",
    "    return int(str(bin(bytes_array[38]))[2:]+str(bin(bytes_array[41]))[2:],2), bytes_array[49]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List of numpy array, each position contains a patient's array of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_store_data (dataset_folder, sample_rate) :\n",
    "    path = os.path.join(os.getcwd(), dataset_folder)\n",
    "    os.chdir(dataset_folder)\n",
    "    \n",
    "    patients = [d for d in os.listdir() if os.path.isdir(d)]\n",
    "    patients.sort()\n",
    "    \n",
    "    for p in patients:\n",
    "        os.chdir(p)\n",
    "        print('Reading data of patient', p)\n",
    "        l=[]\n",
    "        \n",
    "        # for each patient specify the edf files and the seizure files\n",
    "        edf = [f for f in os.listdir() if os.path.isfile(f) and f.endswith('edf')]\n",
    "        edf.sort()\n",
    "        seizures = [f for f in os.listdir() if os.path.isfile(f) and f.endswith('seizures')]\n",
    "        seizures.sort()\n",
    "\n",
    "        # edf files contain 23 columns/channels and a 24th column that indicates the seizure\n",
    "        arr = np.zeros((1,24))\n",
    "        for e in edf:\n",
    "            sigbufs = readEdfFile(e)\n",
    "            print('Reading data file', e, 'with', sigbufs.shape[0], 'records')\n",
    "            sigbufs = np.append(sigbufs, np.zeros((sigbufs.shape[0],1)), axis=1)\n",
    "            if seizures and seizures[0].startswith(e):\n",
    "                (start, length) = get_seizure_period(seizures[0])\n",
    "                print('Reading seizure file', seizures[0], ': (start =',start,'sec, length =',length,'sec) or', '(start =',start*sample_rate,'record, end =',(start+length)*sample_rate,'record)')\n",
    "                for i in range(start*sample_rate, (start+length)*sample_rate+1): sigbufs[i][23] = 1\n",
    "                seizures.pop(0)\n",
    "            arr = np.concatenate([arr, sigbufs])\n",
    "        arr = np.delete(arr, 22, axis=1)\n",
    "        l.append(arr)\n",
    "        os.chdir('..')\n",
    "    os.chdir('..')\n",
    "    \n",
    "    del path, patients, p, edf, seizures, arr, e, sigbufs, start, length, i\n",
    "    \n",
    "    df = pd.DataFrame(np.concatenate(l), columns=['FP1-F7','F7-T7','T7-P7', 'P7-O1','FP1-F3', 'F3-C3', 'C3-P3', 'P3-O1', 'FP2-F4','F4-C4', 'C4-P4', 'P4-O2', 'FP2-F8', 'F8-T8', 'T8-P8', 'P8-O2','FZ-CZ', 'CZ-PZ', 'P7-T7', 'T7-FT9', 'FT9-FT10', 'FT10-T8', 'seizure'])\n",
    "    df.drop(0, inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Left and Right Hemispheric Channel Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leftRightHemisphericChannels(df):\n",
    "    ndf = pd.DataFrame()\n",
    "    ndf['AvgLeftHemisphere'] = (df['F3-C3']+df['C3-P3'])/2\n",
    "    ndf['AvgRightHemisphere'] = (df['F4-C4']+df['C4-P4'])/2\n",
    "    ndf['seizure'] = df['seizure']\n",
    "    return ndf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Domain Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "https://stackoverflow.com/questions/30272538/python-code-for-counting-number-of-zero-crossings-in-an-array\n",
    "https://stackoverflow.com/questions/5613244/root-mean-square-in-numpy-and-complications-of-matrix-and-arrays-of-numpy\n",
    "'''\n",
    "def ComputeTimeDomainFeatures(signal):\n",
    "    mean = np.mean(signal)\n",
    "    var = np.var(signal)\n",
    "    skew = stats.skew(signal)\n",
    "    kurt = stats.kurtosis(signal)\n",
    "    std = np.std(signal)\n",
    "    median = np.median(signal)\n",
    "    zcr = ((signal[:-1] * signal[1:]) < 0).sum()\n",
    "    cv = stats.variation(signal)\n",
    "    rms = np.sqrt(signal.dot(signal)/signal.size)\n",
    "    p2p = signal.max() - signal.min()\n",
    "    sampEn = entropy.sample_entropy(signal, 1)[0]\n",
    "    return mean, var, skew, kurt, std, median, zcr, cv, rms, p2p, sampEn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Compute the average bandpower of an EEG signal\n",
    "https://raphaelvallat.com/bandpower.html\n",
    "'''\n",
    "def psd(x, fs, win):\n",
    "    bands = [0.5, 4, 8, 12, 30, 100]\n",
    "    freqs, psd = signal.welch(x, fs, nperseg=win)\n",
    "    avg_power=[]\n",
    "    while len(bands)>1:\n",
    "        idx = np.logical_and(freqs >= bands[0], freqs <= bands[1])\n",
    "        power_simps = integrate.simps(psd[idx], dx=bands[1]-bands[0])\n",
    "        avg_power.append(power_simps)\n",
    "        bands=np.copy(bands[1:])\n",
    "    for p in avg_power:\n",
    "        yield p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ComputeCorrelation (left, right):\n",
    "    return abs(np.correlate(left, right, 'full')).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def featureExtraction (ndf, sample_rate, step):\n",
    "    print('Computing Features...')\n",
    "    ft = pd.DataFrame()\n",
    "    c = 0\n",
    "    for i in range (0, ndf.shape[0], step):\n",
    "        print('Compute features for chunk from', i, 'to', i+step)\n",
    "        temp = ndf.iloc[i:i+step]\n",
    "        left = np.array(temp['AvgLeftHemisphere'])\n",
    "        right = np.array(temp['AvgRightHemisphere'])\n",
    "\n",
    "        # Time Domain Features\n",
    "        ft.loc[c,'Lmean'], ft.loc[c,'Lvar'], ft.loc[c,'Lskew'],ft.loc[c,'Lkurt'], ft.loc[c,'Lstd'], ft.loc[c,'Lmedian'], ft.loc[c,'Lzcr'], ft.loc[c,'Lcv'], ft.loc[c,'Lrms'], ft.loc[c,'Lp2p'],ft.loc[c,'LsampEn'] = ComputeTimeDomainFeatures(left)\n",
    "        ft.loc[c,'Rmean'], ft.loc[c,'Rvar'], ft.loc[c,'Rskew'],ft.loc[c,'Rkurt'], ft.loc[c,'Rstd'], ft.loc[c,'Rmedian'], ft.loc[c,'Rzcr'], ft.loc[c,'Rcv'], ft.loc[c,'Rrms'], ft.loc[c,'Rp2p'],ft.loc[c,'RsampEn'] = ComputeTimeDomainFeatures(right)\n",
    "\n",
    "        # Frequency Domain Features\n",
    "        ft.loc[c,'LdeltaPower'], ft.loc[c,'LthetaPower'], ft.loc[c,'LalphaPower'], ft.loc[c,'LbetaPower'], ft.loc[c,'LgammaPower'] = psd(left, sample_rate, 4*sample_rate)\n",
    "        ft.loc[c,'RdeltaPower'], ft.loc[c,'RthetaPower'], ft.loc[c,'RalphaPower'], ft.loc[c,'RbetaPower'], ft.loc[c,'RgammaPower'] = psd(right, sample_rate, 4*sample_rate)\n",
    "\n",
    "        \n",
    "        # Correlation Features\n",
    "        ft.loc[c, 'corr'] = ComputeCorrelation(left, right)\n",
    "\n",
    "        ft.loc[c, 'seizure'] = temp['seizure'].value_counts().idxmax()\n",
    "        c = c + 1\n",
    "    print('Feature extraction completed')\n",
    "    return ft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureNormalization(ft):\n",
    "    return pd.DataFrame(preprocessing.scale(ft), columns=ft.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dimentionalityReduction(features, pca_tolerance):\n",
    "    pca = decomposition.PCA(n_components = pca_tolerance).fit(features)\n",
    "    features = pd.DataFrame(pca.transform(features))\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification performance indexes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "TP : the number of segments that are correctly identified as ictal (x_true == x_pred == 1)\n",
    "TN : the number of EEG segments that are correctly classified as non-ictal (x_true == x_pred == 0)\n",
    "FP : the number of EEG segments that are incorrectly classified as ictal (x_true == 0 && x_pred == 1)\n",
    "FN : the segments that are incorrectly classified as non-ictal (x_true == 1 && x_pred == 0)\n",
    "'''\n",
    "def metrics (true_arr, pred_arr):\n",
    "    (tn, fp, fn, tp) = metr.confusion_matrix(true_arr, pred_arr).ravel()\n",
    "    acc = metr.accuracy_score(true_arr, pred_arr)                  # Accuracy\n",
    "    snv = tp/(tp + fn)                                             # Sensitivity or True Positive Rate (TPR)\n",
    "    spc = tn/(tn + fp)                                             # Specificity or True Negative Rate (TNR)\n",
    "    ppv = tp/(tp + fp)                                             # Precision or Positive Predictive Value (PPV)\n",
    "    f1 = metrics.f1_score(true_arr, pred_arr)                      # F1 score\n",
    "    mcc = metrics.matthews_corrcoef(true_arr, pred_arr)            # Matthews Correlation Coefficient\n",
    "    kappa = metrics.cohen_kappa_score(true_arr, pred_arr)          # Cohen’s Kappa\n",
    "    return acc, snv, spc, ppv, f1, mcc, kappa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_rate = 256\n",
    "time_window = 5\n",
    "step = time_window*sample_rate\n",
    "pca_tolerance = 0.95\n",
    "dataset = 'TestData' # 'CHB-MIT-Dataset'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seazure Detection Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_and_store_data(dataset, sample_rate)\n",
    "ndf = leftRightHemisphericChannels(df)\n",
    "features = pd.DataFrame(featureExtraction (ndf, sample_rate, step))\n",
    "features = featureNormalization(features)\n",
    "features = dimentionalityReduction(features, pca_tolerance)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
